{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Call the function with your chosen seed\n",
    "seed_everything(seed=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/sayem/Desktop/deepHSI\")  # Adjust to your project root path\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Custom module imports\n",
    "from src.dataset.components.hyperspectral_dataset import HyperspectralDataset\n",
    "from src.dataset.components.utils import *\n",
    "from src.dataset.medical_datasets.bloodHSI import BloodDetectionHSIDataModule\n",
    "from src.dataset.remote_sensing_datasets.paviaC import PaviaCDataModule\n",
    "from src.dataset.remote_sensing_datasets.ksc import KSCDataModule\n",
    "from src.models.hsi_classification_module import HSIClassificationLitModule\n",
    "from src.models.components.simple_dense_net import HSIFCModel\n",
    "\n",
    "# PyTorch and metrics imports\n",
    "import torch\n",
    "from torchmetrics import Precision, Recall, F1Score\n",
    "\n",
    "# Importing from `lightning` instead of `pytorch_lightning`\n",
    "import lightning as L\n",
    "# from lightning import Trainer\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Fri May 20 18:26:22 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'pavia_gt': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Path to the .mat file\n",
    "mat_file_path = '/home/sayem/Desktop/deepHSI/data/PaviaC/PaviaC/Pavia_gt.mat'\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat(mat_file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the data module\n",
    "data_dir = '/home/sayem/Desktop/deepHSI/data'  # Specify the directory where you want the data to be downloaded\n",
    "\n",
    "# Include 'batch_size', 'num_workers', and 'num_classes' within the hyperparams dictionary\n",
    "hyperparams = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 24,\n",
    "    \"patch_size\": 10, \n",
    "    \"center_pixel\": True, \n",
    "    \"supervision\": \"full\",\n",
    "    \"num_classes\": 10  # Define the number of classes in your dataset\n",
    "}\n",
    "\n",
    "# Assuming YourModel is defined elsewhere and num_classes is known\n",
    "input_channels = 102\n",
    "\n",
    "# Define custom metrics for the classification task using the updated hyperparams\n",
    "custom_metrics = {\n",
    "    \"precision\": Precision(num_classes=hyperparams[\"num_classes\"], average='macro', task='multiclass'),\n",
    "    \"recall\": Recall(num_classes=hyperparams[\"num_classes\"], average='macro', task='multiclass'),\n",
    "    \"f1\": F1Score(num_classes=hyperparams[\"num_classes\"], average='macro', task='multiclass')\n",
    "}\n",
    "\n",
    "model = HSIFCModel(\n",
    "    input_channels=input_channels,\n",
    "    patch_size=hyperparams[\"patch_size\"],  # Use patch_size from hyperparams\n",
    "    n_classes=hyperparams[\"num_classes\"],  # Use num_classes from hyperparams\n",
    "    dropout=True\n",
    ")\n",
    "\n",
    "from lightning.pytorch.callbacks import LambdaCallback\n",
    "\n",
    "# Initialize the HSIClassificationLitModule with the model and other hyperparameters\n",
    "hsi_module = HSIClassificationLitModule(\n",
    "    net=model,  # Your model\n",
    "    optimizer='Adam',  # Using the Adam optimizer\n",
    "    optimizer_params={\"lr\": 1e-5},  # Initial learning rate set to 1e-5\n",
    "    num_classes=hyperparams[\"num_classes\"],  # Number of classes from your hyperparameters\n",
    "    custom_metrics=custom_metrics,  # Custom metrics if any\n",
    "    scheduler=None, # 'ReduceLROnPlateau',  # Using the ReduceLROnPlateau learning rate scheduler\n",
    "    scheduler_params={}\n",
    ")\n",
    "\n",
    "# # Initialize the PyTorch Lightning Trainer\n",
    "# trainer = Trainer(max_epochs=10, precision='16-mixed', accelerator='gpu', devices=1)\n",
    "max_epochs = 200\n",
    "\n",
    "# Initialize the PaviaCDataModule with the updated arguments\n",
    "pavia_c_datamodule = PaviaCDataModule(\n",
    "    data_dir=data_dir,\n",
    "    hyperparams=hyperparams  # Pass hyperparams which now includes num_classes\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stop_callback = L.pytorch.callbacks.EarlyStopping(\n",
    "    monitor='val/f1',  # Specify the metric to monitor\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,  # Whether to print logs to stdout\n",
    "    mode='max',  # In 'min' mode, training will stop when the quantity monitored has stopped decreasing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Specify the directory to save checkpoints\n",
    "ckpt_dir = Path('/home/sayem/Desktop/deepHSI/notebooks/ckpt')\n",
    "\n",
    "# Function to clear directory\n",
    "def clear_directory(path: Path):\n",
    "    if path.is_dir():\n",
    "        for item in path.iterdir():\n",
    "            if item.is_dir():\n",
    "                clear_directory(item)\n",
    "                item.rmdir()\n",
    "            else:\n",
    "                item.unlink()\n",
    "    else:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clear and/or create the log and checkpoint directories\n",
    "clear_directory(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val/f1',  # Metric to monitor\n",
    "    dirpath=str(ckpt_dir),  # Convert Path object to string, Directory to save checkpoints\n",
    "    filename='best-checkpoint-{epoch:02d}-{val/f1:.2f}',  # Checkpoint file name\n",
    "    save_top_k=1,  # Save only the best checkpoint\n",
    "    mode='max',  # 'max' because we want to maximize 'val/f1'\n",
    "    verbose=True,  # Print a message when a new best is found\n",
    "    auto_insert_metric_name=False  # Prevents metric names being inserted into filename automatically\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sayem/anaconda3/envs/deepHSI/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'PaviaC' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PyTorch Lightning Trainer with fast_dev_run enabled\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=False,  # Enable fast_dev_run\n",
    "    precision='16-mixed',  # Use 16-bit precision\n",
    "    accelerator='auto',  # Specify the accelerator as GPU\n",
    "    max_epochs = max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    callbacks=[early_stop_callback, model_checkpoint],\n",
    ")\n",
    "\n",
    "# # Prepare and set up the data module\n",
    "pavia_c_datamodule.prepare_data()\n",
    "pavia_c_datamodule.setup(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datamodule = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Prepare and set up the data module\n",
    "# # pavia_c_datamodule.prepare_data()\n",
    "# # pavia_c_datamodule.setup(stage='test')  # Use 'test' stage to prepare the test data\n",
    "\n",
    "# # # Assuming you have a LightningModule (model) defined and initialized\n",
    "# # # model = YourModel()\n",
    "\n",
    "# # # Evaluate the model on the test set before training (untrained performance)\n",
    "# trainer.test(model, datamodule=pavia_c_datamodule)\n",
    "\n",
    "# # # After testing the untrained model, you can proceed to train the model\n",
    "# # pavia_c_datamodule.setup(stage='fit')  # If needed, set up for training\n",
    "# # trainer.fit(model, datamodule=pavia_c_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "\n",
    "# Create a Tuner object\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Use the lr_find method from the Tuner\n",
    "lr_finder = tuner.lr_find(\n",
    "    model=hsi_module,\n",
    "    datamodule=pavia_c_datamodule,\n",
    "    min_lr=1e-8,  # minimum learning rate to investigate\n",
    "    max_lr=1e-1,  # maximum learning rate to investigate\n",
    "    num_training=200,  # number of learning rates to test\n",
    "    mode='exponential',  # increases the learning rate exponentially\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "\n",
    "# Get the suggested learning rate\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "# Update the learning rate in your model's or optimizer's configuration\n",
    "hsi_module.hparams.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scale_batch_size method from the Tuner\n",
    "new_batch_size = tuner.scale_batch_size(\n",
    "    model=hsi_module,\n",
    "    datamodule=pavia_c_datamodule,\n",
    "    mode='power',  # 'power' or 'binsearch' for search strategy\n",
    "    steps_per_trial=3,  # Number of steps to run with a given batch size\n",
    "    init_val=hyperparams['batch_size'],  # Start from the current batch size in hyperparams\n",
    "    max_trials=25,  # Max number of tries before stopping\n",
    "    batch_arg_name='batch_size'  # The attribute name in model or datamodule that stores the batch size\n",
    ")\n",
    "\n",
    "# Update the batch size in hyperparameters\n",
    "hsi_module.hparams['batch_size'] = new_batch_size\n",
    "pavia_c_datamodule.batch_size = new_batch_size  # Make sure to update the data module's batch size as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the train dataset from the data module\n",
    "# trainer.fit(hsi_module, pavia_c_datamodule.train_dataloader())  \n",
    "trainer.fit(hsi_module, datamodule=pavia_c_datamodule)\n",
    "# Use train_dataloader() instead of train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepHSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
