{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)  # This will print the list of paths where Python looks for modules\n",
    "\n",
    "# Try importing your package\n",
    "import deepHSI\n",
    "import sys\n",
    "print(sys.path)  # This will print the list of paths where Python looks for modules\n",
    "\n",
    "# Try importing your package\n",
    "import deepHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/sayem/Desktop/deepHSI\")  # Adjust to your project root path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "\n",
    "# Custom module imports\n",
    "from deepHSI.dataset.components.hyperspectral_dataset import HyperspectralDataset\n",
    "from deepHSI.dataset.components.utils import *\n",
    "from deepHSI.dataset.medical_datasets.bloodHSI import BloodDetectionHSIDataModule\n",
    "from deepHSI.dataset.remote_sensing_datasets.ksc import KSCDataModule\n",
    "from deepHSI.dataset.remote_sensing_datasets.paviaC import PaviaCDataModule\n",
    "from deepHSI.models.components.simple_dense_net import HSIFCModel, HSIFCResNetModel\n",
    "from deepHSI.models.hsi_classification_module import HSIClassificationLitModule\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# Importing from `lightning` instead of `pytorch_lightning`\n",
    "import lightning as L\n",
    "\n",
    "# PyTorch and metrics imports\n",
    "import torch\n",
    "from torchmetrics import F1Score, Precision, Recall\n",
    "\n",
    "# from lightning import Trainer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Specify the directory to save checkpoints\n",
    "ckpt_dir = Path(\"/home/sayem/Desktop/deepHSI/notebooks/ckpt\")\n",
    "\n",
    "# Function to clear directory\n",
    "\n",
    "def clear_directory(path: Path):\n",
    "    if path.is_dir():\n",
    "        for item in path.iterdir():\n",
    "            if item.is_dir():\n",
    "                clear_directory(item)\n",
    "                item.rmdir()\n",
    "            else:\n",
    "                item.unlink()\n",
    "    else:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clear and/or create the log and checkpoint directories\n",
    "clear_directory(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Path to the .mat file\n",
    "mat_file_path = \"/home/sayem/Desktop/deepHSI/data/PaviaC/PaviaC/Pavia_gt.mat\"\n",
    "\n",
    "# Load the .mat file\n",
    "data = loadmat(mat_file_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the data module\n",
    "data_dir = \"/home/sayem/Desktop/deepHSI/data\"  # Specify the directory where you want the data to be downloaded\n",
    "\n",
    "# Include 'batch_size', 'num_workers', and 'num_classes' within the hyperparams dictionary\n",
    "hyperparams = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 24,\n",
    "    \"patch_size\": 10,\n",
    "    \"center_pixel\": True,\n",
    "    \"supervision\": \"full\",\n",
    "    \"num_classes\": 10,  # Define the number of classes in your dataset\n",
    "}\n",
    "\n",
    "# Assuming YourModel is defined elsewhere and num_classes is known\n",
    "input_channels = 102\n",
    "\n",
    "# Define custom metrics for the classification task using the updated hyperparams\n",
    "custom_metrics = {\n",
    "    \"precision\": Precision(\n",
    "        num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"\n",
    "    ),\n",
    "    \"recall\": Recall(num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"),\n",
    "    \"f1\": F1Score(num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"),\n",
    "}\n",
    "\n",
    "model = HSIFCModel(\n",
    "    input_channels=input_channels,\n",
    "    patch_size=hyperparams[\"patch_size\"],  # Use patch_size from hyperparams\n",
    "    n_classes=hyperparams[\"num_classes\"],  # Use num_classes from hyperparams\n",
    "    dropout=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '1.0-hsi-initial-data-exploration.ipynb'\n",
    "\n",
    "wandb.login()\n",
    "# Initialize WandbLogger with more control and a meaningful run name\n",
    "wandb_logger = WandbLogger(\n",
    "    name=f\"Run-Baseline\",  # Custom run name with a meaningful trailing name\n",
    "    project=\"PaviaC\",  # Your project name\n",
    "    save_dir=\"/home/sayem/Desktop/deepHSI/notebooks/wandb\",  # Directory to save logs\n",
    "    offline=False,  # Set to True if you want to run offline and upload later\n",
    "    id=None,  # Can set a specific ID for the run, useful for resuming\n",
    "    anonymous=False,  # Set to True to anonymously log data\n",
    "    log_model='all',  # Log all checkpoints during training\n",
    "    # prefix=\"my_experiment_\",  # Prefix for all logged metrics\n",
    "    # Additional Wandb init arguments\n",
    "    tags=[\"Baseline\", \"without-scheduler\"],  # Tags for the run\n",
    "    # group=\"experiment_group\",  # Group under which to organize the run\n",
    "    # notes=\"Testing different architectures on the PaviaC dataset\",  # Notes about the run\n",
    "    # # More kwargs can be added as needed\n",
    ")\n",
    "\n",
    "# add your batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = hyperparams[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "# Hyperparameters from your YAML configuration\n",
    "lr = 0.001  # learning rate\n",
    "weight_decay = 0.0  # weight decay\n",
    "\n",
    "# 'partial' creates a new function that when called will behave like 'torch.optim.Adam'\n",
    "# with the given 'lr' and 'weight_decay' parameters pre-filled\n",
    "optimizer = partial(torch.optim.Adam, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Since 'optimizer_func' is a function created by 'partial', it won't be an instance of 'torch.optim.Optimizer'\n",
    "# Therefore, the isinstance check is not applicable here. We can check if it's callable instead:\n",
    "assert callable(optimizer)\n",
    "\n",
    "\n",
    "scheduler = None # # Define the partial function for ReduceLROnPlateau scheduler with desired parameters\n",
    "\n",
    "scheduler = partial(\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', \n",
    "    cooldown=0, min_lr=0, eps=1e-08\n",
    ")\n",
    "\n",
    "# Since 'scheduler_func' is also a function created by 'partial', we check if it's callable:\n",
    "assert callable(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HSIClassificationLitModule with the model and other hyperparameters\n",
    "hsi_classifier = HSIClassificationLitModule(\n",
    "    net=model,  # Your model instance\n",
    "    optimizer=optimizer,  # The Adam optimizer class from torch.optim\n",
    "    scheduler=scheduler,  # The ReduceLROnPlateau scheduler class from torch.optim.lr_scheduler\n",
    "    loss_fn=torch.nn.functional.cross_entropy,  # Using cross-entropy loss function\n",
    "    num_classes=hyperparams[\"num_classes\"],  # Number of classes from your hyperparameters\n",
    "    custom_metrics=custom_metrics  # Custom metrics dictionary if any\n",
    ")\n",
    "\n",
    "# # Initialize the PyTorch Lightning Trainer\n",
    "# trainer = Trainer(max_epochs=10, precision='16-mixed', accelerator='gpu', devices=1)\n",
    "max_epochs = 200\n",
    "\n",
    "# Initialize the PaviaCDataModule with the updated arguments\n",
    "pavia_c_datamodule = PaviaCDataModule(\n",
    "    data_dir=data_dir, hyperparams=hyperparams  # Pass hyperparams which now includes num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "# Define the EarlyStopping callback\n",
    "early_stop_callback = L.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val/f1\",  # Specify the metric to monitor\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,  # Whether to print logs to stdout\n",
    "    mode=\"max\",  # In 'min' mode, training will stop when the quantity monitored has stopped decreasing\n",
    "    check_on_train_epoch_end=False, \n",
    ")\n",
    "\n",
    "# from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "model_checkpoint = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val/f1\",  # Metric to monitor\n",
    "    dirpath=str(ckpt_dir),  # Convert Path object to string, Directory to save checkpoints\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val/f1:.2f}\",  # Checkpoint file name\n",
    "    save_top_k=1,  # Save only the best checkpoint\n",
    "    mode=\"max\",  # 'max' because we want to maximize 'val/f1'\n",
    "    verbose=True,  # Print a message when a new best is found\n",
    "    auto_insert_metric_name=False,  # Prevents metric names being inserted into filename automatically\n",
    ")\n",
    "\n",
    "rich_pbar_callback = L.pytorch.callbacks.RichProgressBar(\n",
    "    refresh_rate=1,\n",
    "    leave=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepHSI.utils.confusion_matrix_callback import ConfusionMatrixLoggerCallBack\n",
    "\n",
    "confusion_matrix_callback = ConfusionMatrixLoggerCallBack()\n",
    "\n",
    "\n",
    "from deepHSI.utils.lr_finder_callback import InitialLRFinder, DynamicLRFinder\n",
    "\n",
    "# # Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "# dynamic_lr_finder = InitialLRFinder(min_lr=1e-8, max_lr=1, \\\n",
    "#     num_training_steps=100, mode='exponential')\n",
    "\n",
    "# Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "lr_finder_callback = InitialLRFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepHSI.utils.lr_finder_callback import InitialLRFinder, DynamicLRFinder\n",
    "\n",
    "# # Define the epochs at which you want to run the LR finder\n",
    "# milestones = [0, 5, 10]  # For example, at the beginning, 5th epoch, and 10th epoch\n",
    "\n",
    "# # Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "# dynamic_lr_finder = DynamicLRFinder(milestones=milestones, \\\n",
    "#     min_lr=1e-8, max_lr=1, num_training_steps=100, mode='exponential')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepHSI.utils.lr_finder_callback import InitialLRFinder, DynamicLRFinder\n",
    "\n",
    "# # # Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "# # dynamic_lr_finder = InitialLRFinder(min_lr=1e-8, max_lr=1, \\\n",
    "# #     num_training_steps=100, mode='exponential')\n",
    "\n",
    "# # Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "# dynamic_lr_finder = InitialLRFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PyTorch Lightning Trainer with fast_dev_run enabled\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=False,  # Enable fast_dev_run\n",
    "    precision=\"16-mixed\",  # Use 16-bit precision\n",
    "    accelerator=\"auto\",  # Specify the accelerator as GPU\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    callbacks=[lr_finder_callback, early_stop_callback, \\\n",
    "        model_checkpoint, confusion_matrix_callback], # rich_pbar_callback],\n",
    "    logger=wandb_logger,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(hsi_classifier, datamodule=pavia_c_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the scale_batch_size method from the Tuner\n",
    "# new_batch_size = tuner.scale_batch_size(\n",
    "#     model=hsi_module,\n",
    "#     datamodule=pavia_c_datamodule,\n",
    "#     mode='power',  # 'power' or 'binsearch' for search strategy\n",
    "#     steps_per_trial=3,  # Number of steps to run with a given batch size\n",
    "#     init_val=hyperparams['batch_size'],  # Start from the current batch size in hyperparams\n",
    "#     max_trials=25,  # Max number of tries before stopping\n",
    "#     batch_arg_name='batch_size'  # The attribute name in model or datamodule that stores the batch size\n",
    "# )\n",
    "\n",
    "# # Update the batch size in hyperparameters\n",
    "# hsi_module.hparams['batch_size'] = new_batch_size\n",
    "# pavia_c_datamodule.batch_size = new_batch_size  # Make sure to update the data module's batch size as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the train dataset from the data module\n",
    "dictionary = trainer.test(hsi_classifier, pavia_c_datamodule, verbose=True)\n",
    "# trainer.fit(hsi_module, datamodule=pavia_c_datamodule)\n",
    "# Use train_dataloader() instead of train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepHSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
