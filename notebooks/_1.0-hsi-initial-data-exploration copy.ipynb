{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.path)  # This will print the list of paths where Python looks for modules\n",
    "\n",
    "import sys\n",
    "\n",
    "# Try importing your package\n",
    "import deepHSI\n",
    "\n",
    "print(sys.path)  # This will print the list of paths where Python looks for modules\n",
    "\n",
    "# Try importing your package\n",
    "import deepHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/sayem/Desktop/deepHSI\")  # Adjust to your project root path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "\n",
    "# Custom module imports\n",
    "from deepHSI.dataset.components.hyperspectral_dataset import HyperspectralDataset\n",
    "from deepHSI.dataset.components.utils import *\n",
    "from deepHSI.dataset.medical_datasets.bloodHSI import BloodDetectionHSIDataModule\n",
    "from deepHSI.dataset.remote_sensing_datasets.ksc import KSCDataModule\n",
    "from deepHSI.dataset.remote_sensing_datasets.paviaC import PaviaCDataModule\n",
    "from deepHSI.models.components.simple_dense_net import HSIFCModel, HSIFCResNetModel\n",
    "from deepHSI.models.hsi_classification_module import HSIClassificationLitModule\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "# Importing from `lightning` instead of `pytorch_lightning`\n",
    "import lightning as L\n",
    "\n",
    "# PyTorch and metrics imports\n",
    "import torch\n",
    "from torchmetrics import F1Score, Precision, Recall\n",
    "\n",
    "# from lightning import Trainer\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Specify the directory to save checkpoints\n",
    "ckpt_dir = Path(\"/home/sayem/Desktop/deepHSI/notebooks/ckpt\")\n",
    "\n",
    "# Function to clear directory\n",
    "\n",
    "\n",
    "def clear_directory(path: Path):\n",
    "    if path.is_dir():\n",
    "        for item in path.iterdir():\n",
    "            if item.is_dir():\n",
    "                clear_directory(item)\n",
    "                item.rmdir()\n",
    "            else:\n",
    "                item.unlink()\n",
    "    else:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Clear and/or create the log and checkpoint directories\n",
    "clear_directory(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "\n",
    "# # Path to the .mat file\n",
    "# mat_file_path = \"/home/sayem/Desktop/deepHSI/data/PaviaC/PaviaC/Pavia_gt.mat\"\n",
    "\n",
    "# # Load the .mat file\n",
    "# data = loadmat(mat_file_path)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the data module\n",
    "data_dir = \"/home/sayem/Desktop/deepHSI/data\"  # Specify the directory where you want the data to be downloaded\n",
    "\n",
    "# Include 'batch_size', 'num_workers', and 'num_classes' within the hyperparams dictionary\n",
    "hyperparams = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 24,\n",
    "    \"patch_size\": 10,\n",
    "    \"center_pixel\": True,\n",
    "    \"supervision\": \"full\",\n",
    "    \"num_classes\": 14,  # Define the number of classes in your dataset\n",
    "}\n",
    "\n",
    "# Assuming YourModel is defined elsewhere and num_classes is known\n",
    "input_channels = 176\n",
    "\n",
    "# Define custom metrics for the classification task using the updated hyperparams\n",
    "custom_metrics = {\n",
    "    \"precision\": Precision(\n",
    "        num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"\n",
    "    ),\n",
    "    \"recall\": Recall(num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"),\n",
    "    \"f1\": F1Score(num_classes=hyperparams[\"num_classes\"], average=\"macro\", task=\"multiclass\"),\n",
    "}\n",
    "\n",
    "model = HSIFCResNetModel(\n",
    "    input_channels=input_channels,\n",
    "    patch_size=hyperparams[\"patch_size\"],  # Use patch_size from hyperparams\n",
    "    n_classes=hyperparams[\"num_classes\"],  # Use num_classes from hyperparams\n",
    "    dropout=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import wandb\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"1.0-hsi-initial-data-exploration.ipynb\"\n",
    "\n",
    "wandb.login()\n",
    "# Initialize WandbLogger with more control and a meaningful run name\n",
    "wandb_logger = WandbLogger(\n",
    "    name=f\"Run-ResNet50\",  # Custom run name with a meaningful trailing name\n",
    "    project=\"KSC\",  # Your project name\n",
    "    save_dir=\"/home/sayem/Desktop/deepHSI/notebooks/wandb\",  # Directory to save logs\n",
    "    offline=False,  # Set to True if you want to run offline and upload later\n",
    "    id=None,  # Can set a specific ID for the run, useful for resuming\n",
    "    anonymous=False,  # Set to True to anonymously log data\n",
    "    log_model=\"all\",  # Log all checkpoints during training\n",
    "    # prefix=\"my_experiment_\",  # Prefix for all logged metrics\n",
    "    # Additional Wandb init arguments\n",
    "    tags=[\"ResNet50\", \"with-scheduler\"],  # Tags for the run\n",
    "    # group=\"experiment_group\",  # Group under which to organize the run\n",
    "    # notes=\"Testing different architectures on the PaviaC dataset\",  # Notes about the run\n",
    "    # # More kwargs can be added as needed\n",
    ")\n",
    "\n",
    "# add your batch size to the wandb config\n",
    "wandb_logger.experiment.config[\"batch_size\"] = hyperparams[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "# Hyperparameters from your YAML configuration\n",
    "lr = 0.001  # learning rate\n",
    "weight_decay = 0.0  # weight decay\n",
    "\n",
    "# 'partial' creates a new function that when called will behave like 'torch.optim.Adam'\n",
    "# with the given 'lr' and 'weight_decay' parameters pre-filled\n",
    "optimizer = partial(torch.optim.Adam, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Since 'optimizer_func' is a function created by 'partial', it won't be an instance of 'torch.optim.Optimizer'\n",
    "# Therefore, the isinstance check is not applicable here. We can check if it's callable instead:\n",
    "assert callable(optimizer)\n",
    "\n",
    "scheduler = partial(\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    threshold=0.0001,\n",
    "    threshold_mode=\"rel\",\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    "    eps=1e-08,\n",
    ")\n",
    "\n",
    "# scheduler = None\n",
    "# Since 'scheduler_func' is also a function created by 'partial', we check if it's callable:\n",
    "assert callable(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HSIClassificationLitModule with the model and other hyperparameters\n",
    "hsi_classifier = HSIClassificationLitModule(\n",
    "    net=model,  # Your model instance\n",
    "    optimizer=optimizer,  # The Adam optimizer class from torch.optim\n",
    "    scheduler=scheduler,  # The ReduceLROnPlateau scheduler class from torch.optim.lr_scheduler\n",
    "    loss_fn=torch.nn.functional.cross_entropy,  # Using cross-entropy loss function\n",
    "    num_classes=hyperparams[\"num_classes\"],  # Number of classes from your hyperparameters\n",
    "    custom_metrics=custom_metrics,  # Custom metrics dictionary if any\n",
    ")\n",
    "\n",
    "# # Initialize the PyTorch Lightning Trainer\n",
    "# trainer = Trainer(max_epochs=10, precision='16-mixed', accelerator='gpu', devices=1)\n",
    "max_epochs = 200\n",
    "\n",
    "# Initialize the PaviaCDataModule with the updated arguments\n",
    "ksc_datamodule = KSCDataModule(\n",
    "    data_dir=data_dir, hyperparams=hyperparams  # Pass hyperparams which now includes num_classes\n",
    ")\n",
    "\n",
    "# ksc_datamodule.prepare_data()\n",
    "# ksc_datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "# Define the EarlyStopping callback\n",
    "early_stop_callback = L.pytorch.callbacks.EarlyStopping(\n",
    "    monitor=\"val/f1\",  # Specify the metric to monitor\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,  # Whether to print logs to stdout\n",
    "    mode=\"max\",  # In 'min' mode, training will stop when the quantity monitored has stopped decreasing\n",
    "    check_on_train_epoch_end=False,\n",
    ")\n",
    "\n",
    "# from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "model_checkpoint = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val/f1\",  # Metric to monitor\n",
    "    dirpath=str(ckpt_dir),  # Convert Path object to string, Directory to save checkpoints\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val/f1:.2f}\",  # Checkpoint file name\n",
    "    save_top_k=1,  # Save only the best checkpoint\n",
    "    mode=\"max\",  # 'max' because we want to maximize 'val/f1'\n",
    "    verbose=True,  # Print a message when a new best is found\n",
    "    auto_insert_metric_name=False,  # Prevents metric names being inserted into filename automatically\n",
    ")\n",
    "\n",
    "rich_pbar_callback = L.pytorch.callbacks.RichProgressBar(\n",
    "    refresh_rate=1,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "lr_monitor_callback = L.pytorch.callbacks.LearningRateMonitor(logging_interval='epoch') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepHSI.utils.custom_callbacks.confusion_matrix_callback import (\n",
    "    ConfusionMatrixLoggerCallBack,\n",
    ")\n",
    "\n",
    "confusion_matrix_callback = ConfusionMatrixLoggerCallBack()\n",
    "\n",
    "from deepHSI.utils.custom_callbacks.lr_finder_callback import (\n",
    "    DynamicLRFinder,\n",
    "    InitialLRFinder,\n",
    ")\n",
    "\n",
    "from deepHSI.utils.custom_callbacks.batch_size_finder_callback import InitialBatchSizeFinder\n",
    "# # Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "# dynamic_lr_finder = InitialLRFinder(min_lr=1e-8, max_lr=1, \\\n",
    "#     num_training_steps=100, mode='exponential')\n",
    "\n",
    "# Instantiate the DynamicLRFinder callback with the defined milestones\n",
    "lr_finder_callback = InitialLRFinder()\n",
    "batch_finder_callback = InitialBatchSizeFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PyTorch Lightning Trainer with fast_dev_run enabled\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=False,  # Enable fast_dev_run\n",
    "    precision=\"16-mixed\",  # Use 16-bit precision\n",
    "    accelerator=\"auto\",  # Specify the accelerator as GPU\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    callbacks=[\n",
    "        lr_finder_callback,\n",
    "        early_stop_callback,\n",
    "        model_checkpoint,\n",
    "        confusion_matrix_callback,\n",
    "        # batch_finder_callback,\n",
    "        lr_monitor_callback,\n",
    "    ],  # rich_pbar_callback],\n",
    "    logger=wandb_logger,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(hsi_classifier, datamodule=ksc_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the train dataset from the data module\n",
    "dictionary = trainer.test(hsi_classifier, ksc_datamodule, verbose=True)\n",
    "# trainer.fit(hsi_module, datamodule=ksc_datamodule)\n",
    "# Use train_dataloader() instead of train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepHSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
