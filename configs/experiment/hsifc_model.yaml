# @package _global_

# To execute this experiment run:
# python train.py experiment=indianpines_experiment

defaults:
  - override /data: datamodule
  - override /model: model  # Ensure this points to the correct model config file
  # - override /callbacks: default
  # - override /trainer: default

# All parameters below will be merged with parameters from default configurations set above
# This allows you to overwrite only specified parameters

# net_a.yaml
model:
  net:
    _target_: deepHSI.models.architectures.HSIFCModel
    input_channels: 200
    patch_size: 5
    dropout: true
    n_classes: 17


tags: ["idnianpines", "baseline", "without-lr-schedular"]

# seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 0.5


logger:
  wandb:
    _target_: lightning.pytorch.loggers.WandbLogger
    save_dir: "/media/sayem/510B93E12554BBD1/wandb/"
    name: "Run-run_name"
    project: "IndianPines"
    tags: ${tags}
